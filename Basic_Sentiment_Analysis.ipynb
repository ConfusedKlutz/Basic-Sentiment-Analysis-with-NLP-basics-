{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 816060,
          "sourceType": "datasetVersion",
          "datasetId": 429163
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Basic Sentiment Analysis A-Z",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConfusedKlutz/Basic-Sentiment-Analysis-with-NLP-basics-/blob/main/Basic_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'imdb-dataset-sentiment-analysis-in-csv-format:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F429163%2F816060%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240714%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240714T132515Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D74b43406e4962639347c7bb690c5a1c60931ca444f56685b076974366473fa7c5ed95653f731b10384adf480170e5ceb6a6c0fabd47001bfde50cb6c23f39664713d286207ce547d6911095f77b7fbf13c40236245439f494d60b1a860dee5caf8f4a1efa71bce03f45f4cc54dc9ba7556dfd63c8693d5a8f97cb621e3b523b7f9ecaf5f4eb85932e7d961f0bceeb802adf6e7e78efd8f0843c5c4267d2fb18d495b7567ca332d8d6002162d3bb29adf967e4c36acee910be7b5f51ccfe0f566541260fab013e2e2192f8f0d49cd8248eb14f545e48f425c27db101198dc19350acaa4ec80939f2305d41000f807632238cac709e4f9b4a95f29ced549860f84'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "iIYNwD8Xa0_M"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Natural Language Processing (NLP)\n",
        "\n",
        "## 1. What is NLP?\n",
        "\n",
        "- **Definition:** Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language.\n",
        "- **Simple Explanation:** NLP involves teaching computers to understand and communicate using human language, similar to how we interact with each other through speech or text.\n",
        "\n",
        "## 2. Need for NLP\n",
        "\n",
        "- **Understanding Human Language:** For computers to interact effectively with humans, they must be able to comprehend and respond to natural language inputs meaningfully.\n",
        "- **Automating Repetitive Tasks:** NLP can handle tasks such as sorting emails, summarizing documents, or analyzing sentiment in large volumes of text, reducing manual effort and increasing efficiency.\n",
        "- **Enhancing User Experience:** Technologies like virtual assistants, chatbots, and translation services use NLP to provide more intuitive and responsive interactions, improving overall user satisfaction.\n",
        "\n",
        "## 3. Applications of NLP\n",
        "\n",
        "1. **Machine Translation:** Converts text from one language to another, as seen in tools like Google Translate, making communication across languages easier.\n",
        "2. **Sentiment Analysis:** Analyzes and interprets emotions expressed in text, such as social media comments or customer reviews, to gauge public opinion or customer satisfaction.\n",
        "3. **Text Summarization:** Generates concise summaries of lengthy documents, helping users quickly grasp the main points without reading the entire text.\n",
        "4. **Speech Recognition:** Transcribes spoken language into text, enabling voice-activated assistants like Siri or Alexa to understand and respond to verbal commands.\n",
        "5. **Chatbots and Virtual Assistants:** Provides automated responses and support in customer service settings, simulating human-like interactions to address user queries and issues.\n",
        "\n",
        "## 4. Basic Steps in NLP\n",
        "\n",
        "1. **Text Preprocessing:** Involves cleaning and preparing raw text data by removing irrelevant information, correcting errors, and normalizing the text for analysis.\n",
        "2. **Tokenization:** Breaks down text into smaller units, such as words or phrases, to facilitate analysis and processing.\n",
        "3. **Removing Stop Words:** Eliminates common, less informative words (e.g., \"the,\" \"and\") that do not contribute significant meaning to the analysis.\n",
        "4. **Stemming and Lemmatization:** Reduces words to their base or root forms (e.g., \"running\" to \"run\") to standardize and simplify the text data.\n",
        "5. **Vectorization:** Transforms text into numerical representations (vectors) that can be used by machine learning models for further analysis and processing.\n",
        "6. **Model Building:** Involves creating and training machine learning models to perform tasks like classification, sentiment analysis, or translation based on the processed text data.\n",
        "7. **Evaluation:** Measures the performance and accuracy of NLP models using metrics and validation techniques to ensure they meet the required objectives and provide reliable results.\n"
      ],
      "metadata": {
        "id": "883PvD8la0_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy &> /dev/null\n",
        "print(\"Spacy installed successfully\")\n",
        "import spacy\n",
        "# spaCy is a library for natural language processing (NLP) in Python.\n",
        "# spaCy is known for its ease of use, speed, and accuracy in processing and analyzing large amounts of text."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:35:43.78965Z",
          "iopub.execute_input": "2024-07-08T05:35:43.790103Z",
          "iopub.status.idle": "2024-07-08T05:35:56.504206Z",
          "shell.execute_reply.started": "2024-07-08T05:35:43.79007Z",
          "shell.execute_reply": "2024-07-08T05:35:56.502737Z"
        },
        "trusted": true,
        "id": "-d2souvMa0_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk &> /dev/null\n",
        "print(\"NLTK installed successfully\")\n",
        "# NLTK (Natural Language Toolkit) is a library for natural language processing (NLP) in Python.\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:35:56.506773Z",
          "iopub.execute_input": "2024-07-08T05:35:56.507179Z",
          "iopub.status.idle": "2024-07-08T05:36:09.061093Z",
          "shell.execute_reply.started": "2024-07-08T05:35:56.507145Z",
          "shell.execute_reply": "2024-07-08T05:36:09.059729Z"
        },
        "trusted": true,
        "id": "IFA7pYKxa0_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.063015Z",
          "iopub.execute_input": "2024-07-08T05:36:09.063404Z",
          "iopub.status.idle": "2024-07-08T05:36:09.068924Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.06337Z",
          "shell.execute_reply": "2024-07-08T05:36:09.067555Z"
        },
        "trusted": true,
        "id": "B21EOoVUa0_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Punctuations\n",
        "\n",
        "\n",
        "**Importance of Removing Punctuation in NLP**\n",
        "\n",
        "Removing punctuation is a crucial preprocessing step in NLP and text analysis. Here’s why:\n",
        "\n",
        "1. **Improves Tokenization**\n",
        "   - **Consistency:** Ensures words are tokenized uniformly.\n",
        "   - **Normalization:** Treats punctuated variations of words (e.g., \"word,\" \"word.\") as the same token.\n",
        "\n",
        "\n",
        "2. **Reduces Noise**\n",
        "   - **Focus on Content:** Punctuation doesn’t add significant meaning, so removing it helps focus on the actual words.\n",
        "   - **Simplifies Analysis:** Makes tasks like word frequency counts and text classification more straightforward.\n",
        "\n",
        "\n",
        "3. **Enhances Text Mining**\n",
        "   - **Feature Extraction:** Eliminates irrelevant punctuation that can be treated as noise.\n",
        "   - **Word Embeddings:** Ensures only meaningful words contribute to embeddings.\n",
        "\n",
        "\n",
        "4. **Improves Readability and Preprocessing**\n",
        "   - **Uniform Text:** Creates a consistent format for easier handling.\n",
        "   - **Text Cleaning:** Removes inconsistent or excessive punctuation from scraped text.\n",
        "\n",
        "This approach aids in more effective text analysis and enhances model performance.\n"
      ],
      "metadata": {
        "id": "mrsRAIyfa0_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Manual Removal using Python"
      ],
      "metadata": {
        "id": "i5gJfeXya0_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = '''!()-[]{};:\"\\,<>./?@#$%^&*_~`''' # Defining a Variable string with all punctuation signs\n",
        "\n",
        "test_string = \"Hello everyone !! Are you ready to dive deep into NLP (Natural Language Processing)?? \""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.071872Z",
          "iopub.execute_input": "2024-07-08T05:36:09.07226Z",
          "iopub.status.idle": "2024-07-08T05:36:09.083899Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.07223Z",
          "shell.execute_reply": "2024-07-08T05:36:09.082805Z"
        },
        "trusted": true,
        "id": "BtYXUyvIa0_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_string_with_punctuations_removed = \"\"\n",
        "# using a for loop to remove Punctuation marks and redefine string\n",
        "for char in test_string:\n",
        "    if(char not in punctuations):\n",
        "        test_string_with_punctuations_removed += char"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.08558Z",
          "iopub.execute_input": "2024-07-08T05:36:09.086089Z",
          "iopub.status.idle": "2024-07-08T05:36:09.097927Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.086056Z",
          "shell.execute_reply": "2024-07-08T05:36:09.096664Z"
        },
        "trusted": true,
        "id": "NJW-AKYca0_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_string_with_punctuations_removed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.099648Z",
          "iopub.execute_input": "2024-07-08T05:36:09.100106Z",
          "iopub.status.idle": "2024-07-08T05:36:09.110221Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.100069Z",
          "shell.execute_reply": "2024-07-08T05:36:09.109008Z"
        },
        "trusted": true,
        "id": "17ocKKTva0_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Removal using REGEX"
      ],
      "metadata": {
        "id": "WwQ0xGZZa0_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # importing REGEX Module"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.111789Z",
          "iopub.execute_input": "2024-07-08T05:36:09.112149Z",
          "iopub.status.idle": "2024-07-08T05:36:09.120881Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.112122Z",
          "shell.execute_reply": "2024-07-08T05:36:09.119428Z"
        },
        "trusted": true,
        "id": "pWyR3diba0_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_string = \"Okay then !! Let's get into NLP (Natural Language Processing). \"\n",
        "test_string_with_punctuations_removed_using_Regex = re.sub(r'[^\\w\\s]','',test_string)\n",
        "\n",
        "# [^\\w\\s]\n",
        "# [] Defining the word block\n",
        "# ^ Not\n",
        "# \\w word character\n",
        "# \\s space character"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.12211Z",
          "iopub.execute_input": "2024-07-08T05:36:09.122471Z",
          "iopub.status.idle": "2024-07-08T05:36:09.134147Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.122443Z",
          "shell.execute_reply": "2024-07-08T05:36:09.132824Z"
        },
        "trusted": true,
        "id": "4D81fAAqa0_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_string_with_punctuations_removed_using_Regex)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.136188Z",
          "iopub.execute_input": "2024-07-08T05:36:09.136574Z",
          "iopub.status.idle": "2024-07-08T05:36:09.146893Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.13652Z",
          "shell.execute_reply": "2024-07-08T05:36:09.145335Z"
        },
        "trusted": true,
        "id": "54Z_sOBha0_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenisation\n",
        "\n",
        "Tokenization is a fundamental NLP step that splits text into smaller units called tokens, such as words or sentences. This process converts unstructured text into a structured format for analysis and processing.\n",
        "\n",
        "### Types of Tokenization\n",
        "\n",
        "- **Word Tokenization:** Splits text into individual words.\n",
        "  - *Example:* \"Tokenization is a key step in NLP.\" → `['Tokenization', 'is', 'a', 'key', 'step', 'in', 'NLP', '.']`\n",
        "- **Sentence Tokenization:** Splits text into individual sentences.\n",
        "  - *Example:* \"Tokenization is a key step in NLP. It helps convert text into tokens.\" → `['Tokenization is a key step in NLP.', 'It helps convert text into tokens.']`\n",
        "- **Subword Tokenization:** Splits text into subword units (e.g., Byte-Pair Encoding).\n",
        "  - *Example:* \"unhappiness\" → `['un', 'happiness']`\n",
        "- **Character Tokenization:** Splits text into individual characters.\n",
        "  - *Example:* \"Token\" → `['T', 'o', 'k', 'e', 'n']`\n",
        "- **N-gram Tokenization:** Creates tokens of size n from the text.\n",
        "  - *Example:* \"Tokenization is important.\" → `[(Tokenization, is), (is, important), (important, .)]`\n",
        "\n",
        "### Applications\n",
        "\n",
        "- **Text Classification:** Converts text into features for classification algorithms.\n",
        "- **Information Retrieval:** Aids in indexing and searching text.\n",
        "- **Text Analysis:** Used in sentiment analysis, entity recognition, etc.\n",
        "- **Machine Translation:** Breaks text into manageable units for translation.\n",
        "- **Language Modeling:** Helps predict the next word or sequence.\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- **Ambiguity:** Handling contractions and language ambiguities.\n",
        "- **Special Characters:** Properly managing punctuation and special characters.\n",
        "- **Different Languages:** Adapting tokenization rules for various languages.\n",
        "\n",
        "\n",
        "Tokenization is essential for preparing text data for analysis and enables more advanced text understanding and processing.\n"
      ],
      "metadata": {
        "id": "fZoc7Xbda0_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **With Python**"
      ],
      "metadata": {
        "id": "CfslSxXQa0_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenising_python = \"Lets use our Python basics to tokenise our sentence\"\n",
        "tokenising_python.split(\" \")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.152589Z",
          "iopub.execute_input": "2024-07-08T05:36:09.153308Z",
          "iopub.status.idle": "2024-07-08T05:36:09.163464Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.153265Z",
          "shell.execute_reply": "2024-07-08T05:36:09.162139Z"
        },
        "trusted": true,
        "id": "TOkvI9oJa0_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **using NLTK**"
      ],
      "metadata": {
        "id": "9c87Mjw5a0_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # Punkt is a sentence tokenizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.165502Z",
          "iopub.execute_input": "2024-07-08T05:36:09.167001Z",
          "iopub.status.idle": "2024-07-08T05:36:09.178644Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.166952Z",
          "shell.execute_reply": "2024-07-08T05:36:09.177145Z"
        },
        "trusted": true,
        "id": "j6pSWY0ia0_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenisning_example = \"Now, lets tokenise using a library\"\n",
        "nltk.word_tokenize(tokenisning_example)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.180707Z",
          "iopub.execute_input": "2024-07-08T05:36:09.18123Z",
          "iopub.status.idle": "2024-07-08T05:36:09.194973Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.181189Z",
          "shell.execute_reply": "2024-07-08T05:36:09.192885Z"
        },
        "trusted": true,
        "id": "U0mrHziZa0_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Stop words\n",
        "\n",
        "Removing stop words is a preprocessing step in NLP that involves filtering out common words that don't add significant meaning to text analysis. These include articles, prepositions, conjunctions, and frequent pronouns.\n",
        "\n",
        "### Why Remove Stop Words?\n",
        "\n",
        "- **Reduce Noise:**\n",
        "  - **Focus on Meaningful Words:** Stop words don't contribute much semantic value, so removing them highlights more important words.\n",
        "  - **Improve Accuracy:** Reduces irrelevant information, enhancing text analysis performance.\n",
        "\n",
        "\n",
        "- **Simplify Analysis:**\n",
        "  - **Feature Reduction:** Reduces text data dimensionality, simplifying tasks like classification and clustering.\n",
        "  - **Improve Efficiency:** Fewer tokens lead to faster processing and more efficient algorithms.\n",
        "\n",
        "\n",
        "- **Enhance Model Performance:**\n",
        "  - **Text Classification:** Models trained on cleaner data can better focus on informative features.\n",
        "  - **Search Engines:** Improves search results by concentrating on relevant keywords.\n",
        "\n",
        "### Common Stop Words\n",
        "\n",
        "- **Articles:** a, an, the\n",
        "- **Prepositions:** in, on, at, of\n",
        "- **Conjunctions:** and, or, but\n",
        "- **Pronouns:** he, she, it, they\n",
        "- **Others:** is, are, was, has\n",
        "\n",
        "Removing stop words helps in focusing on the core content of the text, leading to better analysis and model performance.\n"
      ],
      "metadata": {
        "id": "EwPrPET3a0_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.196723Z",
          "iopub.execute_input": "2024-07-08T05:36:09.197979Z",
          "iopub.status.idle": "2024-07-08T05:36:09.21142Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.197932Z",
          "shell.execute_reply": "2024-07-08T05:36:09.209996Z"
        },
        "trusted": true,
        "id": "H1_e3ffda0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_stop_words = \"I trust my preperations for evey aspect of work\"\n",
        "words = text_stop_words.split(\" \")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.214367Z",
          "iopub.execute_input": "2024-07-08T05:36:09.215594Z",
          "iopub.status.idle": "2024-07-08T05:36:09.221166Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.215487Z",
          "shell.execute_reply": "2024-07-08T05:36:09.219836Z"
        },
        "trusted": true,
        "id": "3ffWqgfHa0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_stop_words = [word for word in words if word not in stop_words]\n",
        "no_stop_words"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.222869Z",
          "iopub.execute_input": "2024-07-08T05:36:09.225257Z",
          "iopub.status.idle": "2024-07-08T05:36:09.236281Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.225205Z",
          "shell.execute_reply": "2024-07-08T05:36:09.234843Z"
        },
        "trusted": true,
        "id": "curetcnwa0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words.append(\"work\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.238243Z",
          "iopub.execute_input": "2024-07-08T05:36:09.239493Z",
          "iopub.status.idle": "2024-07-08T05:36:09.24542Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.239439Z",
          "shell.execute_reply": "2024-07-08T05:36:09.243793Z"
        },
        "trusted": true,
        "id": "7xpimLd_a0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_stop_words = [word for word in words if word not in stop_words]\n",
        "no_stop_words"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.247058Z",
          "iopub.execute_input": "2024-07-08T05:36:09.247458Z",
          "iopub.status.idle": "2024-07-08T05:36:09.257461Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.247428Z",
          "shell.execute_reply": "2024-07-08T05:36:09.256184Z"
        },
        "trusted": true,
        "id": "NmXlQrJJa0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "Stemming is a text normalization technique in NLP that reduces words to their root form by stripping suffixes. This process helps standardize text for further analysis.\n",
        "\n",
        "## Why Use Stemming?\n",
        "\n",
        "- **Reduce Variability:**\n",
        "  - **Normalization:** Different forms of a word (e.g., \"running,\" \"runner\") are reduced to a common base form (e.g., \"run\").\n",
        "  - **Improved Matching:** Enhances search and retrieval by standardizing word variations.\n",
        "\n",
        "- **Simplify Analysis:**\n",
        "  - **Feature Reduction:** Reduces the dimensionality of text data, improving machine learning model performance and analysis efficiency.\n",
        "\n",
        "- **Enhance Search and Indexing:**\n",
        "  - **Consistency:** Ensures variations of a word are treated as the same word, benefiting search engines and information retrieval.\n",
        "\n",
        "## How Stemming Works\n",
        "\n",
        "Stemming algorithms remove prefixes and suffixes from words using predefined rules or lookup tables to derive root forms.\n",
        "\n",
        "### Popular Stemming Algorithms\n",
        "\n",
        "- **Porter Stemmer:**\n",
        "  - **Description:** Widely used, applies a set of rules to iteratively remove suffixes.\n",
        "\n",
        "\n",
        "- **Lancaster Stemmer:**\n",
        "  - **Description:** More aggressive, applies more rules, often resulting in shorter stems.\n",
        "\n",
        "\n",
        "- **Snowball Stemmer:**\n",
        "  - **Description:** Improved version of Porter Stemmer, supports multiple languages with more sophisticated rules.\n",
        "\n",
        "## Advantages of Stemming\n",
        "\n",
        "- **Consistency:** Reduces variations to a common base form.\n",
        "- **Efficiency:** Simplifies text, reducing unique tokens and speeding up processing.\n",
        "- **Improved Matching:** Enhances ability to match words with similar meanings.\n",
        "\n",
        "## Disadvantages of Stemming\n",
        "\n",
        "- **Over-Stemming:** May lead to loss of meaning by reducing words to overly simplistic forms.\n",
        "- **Irregular Stems:** Can produce inconsistent or confusing root forms.\n",
        "- **Language Dependence:** Often language-specific and may not work well with all languages.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Stemming normalizes words by reducing them to their base forms, aiding in text simplification and analysis. While it improves efficiency and consistency, it can also result in loss of meaning and irregularities. Understanding these aspects helps in applying stemming effectively in NLP tasks.\n"
      ],
      "metadata": {
        "id": "SgkM1akCa0_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.259274Z",
          "iopub.execute_input": "2024-07-08T05:36:09.259791Z",
          "iopub.status.idle": "2024-07-08T05:36:09.268087Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.259753Z",
          "shell.execute_reply": "2024-07-08T05:36:09.266752Z"
        },
        "trusted": true,
        "id": "yboyG3AIa0_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem_input = \"Stemming breaks words in to their base forms. The working of stemmer is predefined using various algorithms\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.26988Z",
          "iopub.execute_input": "2024-07-08T05:36:09.270379Z",
          "iopub.status.idle": "2024-07-08T05:36:09.28002Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.270338Z",
          "shell.execute_reply": "2024-07-08T05:36:09.278619Z"
        },
        "trusted": true,
        "id": "OzJ8CBF_a0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stem_input = nltk.word_tokenize(stem_input)\n",
        "for word in stem_input:\n",
        "    print(stemmer.stem(word))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.281491Z",
          "iopub.execute_input": "2024-07-08T05:36:09.281927Z",
          "iopub.status.idle": "2024-07-08T05:36:09.293951Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.281896Z",
          "shell.execute_reply": "2024-07-08T05:36:09.292874Z"
        },
        "trusted": true,
        "id": "5yKlWFL2a0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatisation\n",
        "\n",
        "Lemmatization is a text normalization technique in NLP that reduces words to their base or dictionary form, known as a lemma. Unlike stemming, which applies heuristic rules, lemmatization uses a linguistic approach considering the word's context and part of speech.\n",
        "\n",
        "### Why Use Lemmatization?\n",
        "\n",
        "- **Meaningful Base Forms:**\n",
        "  - **Context-Aware:** Considers the word's context and part of speech to derive valid dictionary words.\n",
        "  - **Improved Accuracy:** Preserves the semantic meaning of words.\n",
        "\n",
        "\n",
        "- **Enhanced Text Analysis:**\n",
        "  - **Consistency:** Reduces different inflections of a word to a common base form.\n",
        "   - **Better Representation:** Creates more meaningful text representations by retaining semantic integrity.\n",
        "\n",
        "\n",
        "- **Search and Retrieval:**\n",
        "  - **Effective Matching:** Helps match word variations, benefiting search engines and information retrieval systems.\n",
        "\n",
        "### How Lemmatization Works\n",
        "\n",
        "Lemmatization algorithms use dictionaries or rules to find the base form of a word, involving:\n",
        "\n",
        "- **Part-of-Speech Tagging:** Identifies the grammatical category of the word (e.g., noun, verb) to determine its lemma.\n",
        "- **Contextual Analysis:** Considers the word's usage context for accurate base form determination.\n",
        "\n",
        "### Popular Lemmatization Algorithms\n",
        "\n",
        "- **WordNet Lemmatizer:**\n",
        "  - **Description:** Uses WordNet lexical database for base forms based on part of speech.\n",
        "  \n",
        "  \n",
        "- **spaCy Lemmatizer:**\n",
        "  - **Description:** Part of spaCy's NLP pipeline, providing advanced lemmatization.\n",
        "  \n",
        "## Advantages of Lemmatization\n",
        "\n",
        "- **Contextual Accuracy:** Maintains semantic meaning and provides accurate base forms.\n",
        "- **Reduced Variability:** Converts different word forms to a standardized form.\n",
        "- **Better for NLP Models:** Offers a more interpretable text representation, improving model performance.\n",
        "\n",
        "## Disadvantages of Lemmatization\n",
        "\n",
        "- **Computational Complexity:** More resource-intensive compared to stemming.\n",
        "- **Requires POS Tagging:** Accurate lemmatization often needs part-of-speech tagging.\n",
        "- **Dependency on Lexical Resources:** Effectiveness depends on the quality of lexical resources like WordNet.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Lemmatization reduces words to their dictionary forms, considering linguistic rules and context. It improves text analysis by preserving meaning and providing a more accurate representation, though it can be more complex and resource-demanding than stemming.\n"
      ],
      "metadata": {
        "id": "PqK1AZt6a0_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:09.29529Z",
          "iopub.execute_input": "2024-07-08T05:36:09.295721Z",
          "iopub.status.idle": "2024-07-08T05:36:10.249467Z",
          "shell.execute_reply.started": "2024-07-08T05:36:09.295691Z",
          "shell.execute_reply": "2024-07-08T05:36:10.248377Z"
        },
        "trusted": true,
        "id": "2X-fgSB0a0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:10.251062Z",
          "iopub.execute_input": "2024-07-08T05:36:10.251508Z",
          "iopub.status.idle": "2024-07-08T05:36:11.133991Z",
          "shell.execute_reply.started": "2024-07-08T05:36:10.251477Z",
          "shell.execute_reply": "2024-07-08T05:36:11.132882Z"
        },
        "trusted": true,
        "id": "qxeRx8OKa0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.135111Z",
          "iopub.execute_input": "2024-07-08T05:36:11.135489Z",
          "iopub.status.idle": "2024-07-08T05:36:11.140349Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.13546Z",
          "shell.execute_reply": "2024-07-08T05:36:11.139272Z"
        },
        "trusted": true,
        "id": "A8gRaZkta0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemm_input = \"Stemming breaks words in to their base forms. The working of stemmer is predefined using various algorithms\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.141691Z",
          "iopub.execute_input": "2024-07-08T05:36:11.142054Z",
          "iopub.status.idle": "2024-07-08T05:36:11.153442Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.142026Z",
          "shell.execute_reply": "2024-07-08T05:36:11.152247Z"
        },
        "trusted": true,
        "id": "8fgs547ea0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens = nltk.word_tokenize(lemm_input)\n",
        "nltk_tokens_str = ' '.join(nltk_tokens)\n",
        "print(nltk_tokens)\n",
        "\n",
        "# Process the string with spaCy\n",
        "doc = nlp(nltk_tokens_str)\n",
        "spacy_lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"Lemmatized tokens using spaCy:\", spacy_lemmatized_tokens)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.154746Z",
          "iopub.execute_input": "2024-07-08T05:36:11.155577Z",
          "iopub.status.idle": "2024-07-08T05:36:11.182042Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.155545Z",
          "shell.execute_reply": "2024-07-08T05:36:11.180896Z"
        },
        "trusted": true,
        "id": "QnyExBWja0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words (BoW)\n",
        "\n",
        "Bag of Words (BoW) is a fundamental method used in natural language processing (NLP) to convert text data into numerical features. It simplifies text by representing it as a fixed-length vector of word counts or frequencies, enabling its use in various machine learning tasks.\n",
        "\n",
        "### How Bag of Words Works\n",
        "\n",
        "1. **Tokenization**:\n",
        "   - **Process**: Break the text into individual words or tokens. Example: \"The cat sat on the mat\" → `['The', 'cat', 'sat', 'on', 'the', 'mat']`.\n",
        "\n",
        "\n",
        "2. **Vocabulary Creation**:\n",
        "   - **Process**: Create a vocabulary of all unique tokens from the corpus. Each unique word is assigned an index.\n",
        "\n",
        "\n",
        "3. **Feature Vector Creation**:\n",
        "   - **Process**: Represent each document as a vector, where each dimension corresponds to a word in the vocabulary, with values indicating word counts or frequencies.\n",
        "\n",
        "### Example\n",
        "\n",
        "For a simple corpus with three documents:\n",
        "\n",
        "1. \"The cat sat on the mat.\"\n",
        "2. \"The dog barked at the cat.\"\n",
        "3. \"The cat chased the dog.\"\n",
        "\n",
        "**Step-by-Step Process:**\n",
        "\n",
        "1. **Tokenization**:\n",
        "   - Document 1: `['The', 'cat', 'sat', 'on', 'the', 'mat']`\n",
        "   - Document 2: `['The', 'dog', 'barked', 'at', 'the', 'cat']`\n",
        "   - Document 3: `['The', 'cat', 'chased', 'the', 'dog']`\n",
        "\n",
        "\n",
        "2. **Vocabulary Creation**:\n",
        "   - Vocabulary: `['The', 'cat', 'sat', 'on', 'mat', 'dog', 'barked', 'at', 'chased']`\n",
        "\n",
        "\n",
        "3. **Feature Vector Creation**:\n",
        "   - Document 1: `[2, 1, 1, 1, 1, 0, 0, 0, 0]`\n",
        "   - Document 2: `[2, 1, 0, 0, 0, 1, 1, 1, 0]`\n",
        "   - Document 3: `[2, 1, 0, 0, 0, 1, 0, 0, 1]`\n",
        "\n",
        "### Advantages of Bag of Words\n",
        "\n",
        "1. **Simplicity**:\n",
        "   - **Easy to Implement**: BoW is straightforward and a good starting point for text classification.\n",
        "\n",
        "\n",
        "2. **Effectiveness**:\n",
        "   - **Works Well for Many Tasks**: Effective for text classification and sentiment analysis.\n",
        "\n",
        "\n",
        "3. **Feature Extraction**:\n",
        "   - **Generates Numerical Data**: Converts text data into numerical features for machine learning algorithms.\n",
        "\n",
        "### Disadvantages of Bag of Words\n",
        "\n",
        "1. **Loss of Context**:\n",
        "   - **Ignores Word Order**: Does not capture word order or context.\n",
        "\n",
        "2. **High Dimensionality**:\n",
        "   - **Large Feature Vectors**: Vocabulary size can lead to high-dimensional and sparse vectors.\n",
        "\n",
        "3. **Vocabulary Size**:\n",
        "   - **Fixed Size**: May cause data sparsity in large corpora.\n",
        "\n",
        "4. **No Semantics**:\n",
        "   - **Lacks Meaning**: Does not capture semantic relationships between words."
      ],
      "metadata": {
        "id": "8DqojKULa0_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn. feature_extraction.text import CountVectorizer\n",
        "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
        "success way to resolve text related problem and outperform \\ Bag of Words ( BOW ). Indeed, BoW introduced limitations \\ large feature dimension, sparse representation etc.\"\n",
        "count_vec = CountVectorizer ()\n",
        "count_occurs = count_vec.fit_transform([doc])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.183449Z",
          "iopub.execute_input": "2024-07-08T05:36:11.183797Z",
          "iopub.status.idle": "2024-07-08T05:36:11.191401Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.183768Z",
          "shell.execute_reply": "2024-07-08T05:36:11.190242Z"
        },
        "trusted": true,
        "id": "6jgCvbETa0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vec = CountVectorizer()\n",
        "count_occurs = count_vec.fit_transform([doc])\n",
        "count_occur_df = pd.DataFrame(\n",
        "    {'Word': count_vec.get_feature_names_out(), 'Count': count_occurs.toarray().tolist()[0]}\n",
        ")\n",
        "count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
        "print(count_occur_df.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.193222Z",
          "iopub.execute_input": "2024-07-08T05:36:11.194077Z",
          "iopub.status.idle": "2024-07-08T05:36:11.209672Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.19402Z",
          "shell.execute_reply": "2024-07-08T05:36:11.208501Z"
        },
        "trusted": true,
        "id": "AQ5ZM6X9a0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "x = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(x.toarray())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.216963Z",
          "iopub.execute_input": "2024-07-08T05:36:11.217802Z",
          "iopub.status.idle": "2024-07-08T05:36:11.233Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.21776Z",
          "shell.execute_reply": "2024-07-08T05:36:11.231851Z"
        },
        "trusted": true,
        "id": "Yrrwx9XGa0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "\n",
        "vectorizer_n = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
        "x = vectorizer_n.fit_transform(corpus)\n",
        "print(vectorizer_n.get_feature_names_out())\n",
        "print(x.toarray())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.234249Z",
          "iopub.execute_input": "2024-07-08T05:36:11.234618Z",
          "iopub.status.idle": "2024-07-08T05:36:11.258457Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.23459Z",
          "shell.execute_reply": "2024-07-08T05:36:11.257109Z"
        },
        "trusted": true,
        "id": "gxeaJ2dZa0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "TF-IDF is a statistical measure used in natural language processing (NLP) and information retrieval to assess the importance of a word in a document relative to a collection of documents or corpus. It combines two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
        "\n",
        "### How TF-IDF Works\n",
        "\n",
        "#### 1. Term Frequency (TF)\n",
        "- **Definition**: Measures the frequency of a term (word) in a document.\n",
        "- **Calculation**:\n",
        "  \\[\n",
        "  \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
        "  \\]\n",
        "\n",
        "#### 2. Inverse Document Frequency (IDF)\n",
        "- **Definition**: Measures the importance of a term across a collection of documents.\n",
        "- **Calculation**:\n",
        "  \\[\n",
        "  \\text{IDF}(t, D) = \\log \\left(\\frac{N}{|\\{d \\in D : t \\in d\\}|} + 1 \\right) + 1\n",
        "  \\]\n",
        "  - Where \\( N \\) is the total number of documents in the corpus.\n",
        "  - \\( |\\{d \\in D : t \\in d\\}| \\) is the number of documents containing the term \\( t \\).\n",
        "\n",
        "#### 3. TF-IDF Calculation\n",
        "- **Definition**: Quantifies the relevance of a term in a document by multiplying its TF value by its IDF value.\n",
        "- **Calculation**:\n",
        "  \\[\n",
        "  \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "  \\]\n",
        "\n",
        "\n",
        "**Step-by-Step TF-IDF Calculation**:\n",
        "\n",
        "1. **Tokenization**: Tokenize each document into words.\n",
        "2. **TF Calculation**: Compute the term frequency for each term in each document.\n",
        "3. **IDF Calculation**: Compute the inverse document frequency for each term across the corpus.\n",
        "4. **TF-IDF Calculation**: Multiply TF by IDF to get the TF-IDF score for each term in each document.\n",
        "\n",
        "### Advantages of TF-IDF\n",
        "\n",
        "1. **Term Importance**:\n",
        "   - Highlights terms that are specific to a document and less common across the corpus.\n",
        "\n",
        "2. **Flexible Weighting**:\n",
        "   - Adjusts the importance of terms based on their frequency in the document and across the corpus.\n",
        "\n",
        "3. **Reduces Noise**:\n",
        "   - Filters out common terms that appear frequently across all documents.\n",
        "\n",
        "### Disadvantages of TF-IDF\n",
        "\n",
        "1. **Contextual Understanding**:\n",
        "   - Lacks semantic understanding and does not capture the context of language usage.\n",
        "\n",
        "2. **Normalization**:\n",
        "   - Sensitive to document length and term frequency distributions.\n"
      ],
      "metadata": {
        "id": "zGmPTGWEa0_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# Input documents\n",
        "documentA = 'The rain is pouring'\n",
        "documentB = 'The rain has stopped'\n",
        "\n",
        "# Split the documents into words\n",
        "bagOfWordsA = documentA.lower().split()\n",
        "bagOfWordsB = documentB.lower().split()\n",
        "\n",
        "# Create a set of unique words\n",
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
        "\n",
        "# Initialize dictionaries to count word occurrences\n",
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "numOfWordsB = dict.fromkeys(uniqueWords, 0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.259941Z",
          "iopub.execute_input": "2024-07-08T05:36:11.260363Z",
          "iopub.status.idle": "2024-07-08T05:36:11.271081Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.260323Z",
          "shell.execute_reply": "2024-07-08T05:36:11.269874Z"
        },
        "trusted": true,
        "id": "0k3k2hgPa0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count word occurrences in both documents\n",
        "for word in bagOfWordsA:\n",
        "    numOfWordsA[word] += 1\n",
        "\n",
        "for word in bagOfWordsB:\n",
        "    numOfWordsB[word] += 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.272563Z",
          "iopub.execute_input": "2024-07-08T05:36:11.273009Z",
          "iopub.status.idle": "2024-07-08T05:36:11.281773Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.272972Z",
          "shell.execute_reply": "2024-07-08T05:36:11.280687Z"
        },
        "trusted": true,
        "id": "CHjx67O4a0_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Term Frequency (TF)\n",
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bagOfWordsCount)\n",
        "    return tfDict"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.283407Z",
          "iopub.execute_input": "2024-07-08T05:36:11.284382Z",
          "iopub.status.idle": "2024-07-08T05:36:11.294487Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.284343Z",
          "shell.execute_reply": "2024-07-08T05:36:11.293272Z"
        },
        "trusted": true,
        "id": "el0kEMHha0_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF for both documents\n",
        "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
        "tfB = computeTF(numOfWordsB, bagOfWordsB)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.296029Z",
          "iopub.execute_input": "2024-07-08T05:36:11.296428Z",
          "iopub.status.idle": "2024-07-08T05:36:11.304369Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.296395Z",
          "shell.execute_reply": "2024-07-08T05:36:11.303295Z"
        },
        "trusted": true,
        "id": "6LiWJCcla0_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Inverse Document Frequency (IDF)\n",
        "def computeIDF(docList):\n",
        "    idfDict = {}\n",
        "    N = len(docList)\n",
        "\n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for doc in docList:\n",
        "        for word, val in doc.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "\n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict\n",
        "\n",
        "# Compute IDF for the corpus\n",
        "idfs = computeIDF([numOfWordsA, numOfWordsB])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.305874Z",
          "iopub.execute_input": "2024-07-08T05:36:11.306282Z",
          "iopub.status.idle": "2024-07-08T05:36:11.316973Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.306219Z",
          "shell.execute_reply": "2024-07-08T05:36:11.315669Z"
        },
        "trusted": true,
        "id": "6O6brVKBa0_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute TF-IDF\n",
        "def computeTFIDF(tfDict, idfDict):\n",
        "    tfidf = {}\n",
        "    for word, val in tfDict.items():\n",
        "        tfidf[word] = val * idfDict[word]\n",
        "    return tfidf\n",
        "\n",
        "# Compute TF-IDF for both documents\n",
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.318578Z",
          "iopub.execute_input": "2024-07-08T05:36:11.318977Z",
          "iopub.status.idle": "2024-07-08T05:36:11.334317Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.31894Z",
          "shell.execute_reply": "2024-07-08T05:36:11.333184Z"
        },
        "trusted": true,
        "id": "3DZJkz06a0_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "# print(\"TF-IDF for Document A:\", tfidfA)\n",
        "# print(\"TF-IDF for Document B:\", tfidfB)\n",
        "df = pd.DataFrame([tfidfA,tfidfB])\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.335723Z",
          "iopub.execute_input": "2024-07-08T05:36:11.336245Z",
          "iopub.status.idle": "2024-07-08T05:36:11.355172Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.336209Z",
          "shell.execute_reply": "2024-07-08T05:36:11.353873Z"
        },
        "trusted": true,
        "id": "vtYyVOkna0_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Direct use of Model**"
      ],
      "metadata": {
        "id": "Ggl7q3mea0_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"Feature Names:\", feature_names)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "print(\"\\nTF-IDF DataFrame:\\n\", tfidf_df)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.357659Z",
          "iopub.execute_input": "2024-07-08T05:36:11.358233Z",
          "iopub.status.idle": "2024-07-08T05:36:11.377103Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.358193Z",
          "shell.execute_reply": "2024-07-08T05:36:11.375903Z"
        },
        "trusted": true,
        "id": "sGKHZbxka0_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(\"Feature Names:\", feature_names)\n",
        "\n",
        "# Print the document-term matrix\n",
        "print(\"Document-Term Matrix:\\n\", x.toarray())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.378574Z",
          "iopub.execute_input": "2024-07-08T05:36:11.37894Z",
          "iopub.status.idle": "2024-07-08T05:36:11.389947Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.378911Z",
          "shell.execute_reply": "2024-07-08T05:36:11.388685Z"
        },
        "trusted": true,
        "id": "voyqbb-3a0_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "\n",
        "Word2Vec is a technique in natural language processing (NLP) for representing words as dense vectors in a continuous vector space. It captures semantic meanings and relationships between words based on their context in a corpus.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- **Word Embeddings**: Dense vector representations of words.\n",
        "- **Training Objective**: Position similar words close together in the vector space based on context.\n",
        "\n",
        "### Algorithms\n",
        "\n",
        "1. **Continuous Bag of Words (CBOW)**:\n",
        "   - **Objective**: Predict a target word from its context.\n",
        "   - **Approach**: Average context word vectors to predict the target word vector.\n",
        "\n",
        "\n",
        "2. **Skip-gram**:\n",
        "   - **Objective**: Predict context words from a target word.\n",
        "   - **Approach**: Use target word vector to predict context word vectors.\n",
        "\n",
        "### How It Works\n",
        "\n",
        "1. **Tokenization**: Split text into words.\n",
        "2. **Training**: Train CBOW or Skip-gram on a large text corpus to learn word vectors.\n",
        "3. **Embedding Retrieval**: Extract word vectors for NLP tasks.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- **Semantic Similarity**: Captures word relationships (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\").\n",
        "- **Dimensionality Reduction**: Produces dense vectors.\n",
        "- **Contextual Understanding**: Reflects word meanings based on context.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "- **Fixed Context Window Size**: May miss relevant context.\n",
        "- **Computational Cost**: Training can be expensive.\n",
        "- **Out of Vocabulary (OOV)**: Words not seen during training are not represented.\n"
      ],
      "metadata": {
        "id": "Pt8mDppfa0_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "\n",
        "# Tokenize each document in the corpus\n",
        "tokenized_corpus = [nltk.word_tokenize(doc.lower()) for doc in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "word = 'document'\n",
        "if word in model.wv:\n",
        "    vector = model.wv[word]\n",
        "    print(\"Vector for 'document':\", vector)\n",
        "else:\n",
        "    print(f\"'{word}' not found in the model vocabulary.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.391343Z",
          "iopub.execute_input": "2024-07-08T05:36:11.391694Z",
          "iopub.status.idle": "2024-07-08T05:36:11.417826Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.391666Z",
          "shell.execute_reply": "2024-07-08T05:36:11.416613Z"
        },
        "trusted": true,
        "id": "PNSMBdlDa0_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Model for Sentiment Analysis"
      ],
      "metadata": {
        "id": "p8Dy5iJLa0_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "U60uqPCLa0_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv\")\n",
        "test_data = pd.read_csv(\"/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Test.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:11.41928Z",
          "iopub.execute_input": "2024-07-08T05:36:11.419621Z",
          "iopub.status.idle": "2024-07-08T05:36:12.160324Z",
          "shell.execute_reply.started": "2024-07-08T05:36:11.419593Z",
          "shell.execute_reply": "2024-07-08T05:36:12.159234Z"
        },
        "trusted": true,
        "id": "wiR7XqZ5a0_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:12.161725Z",
          "iopub.execute_input": "2024-07-08T05:36:12.1621Z",
          "iopub.status.idle": "2024-07-08T05:36:12.172348Z",
          "shell.execute_reply.started": "2024-07-08T05:36:12.16207Z",
          "shell.execute_reply": "2024-07-08T05:36:12.171302Z"
        },
        "trusted": true,
        "id": "4pG0OZK-a0_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize using NLTK's Punkt tokenizer\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove punctuation and non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:12.17391Z",
          "iopub.execute_input": "2024-07-08T05:36:12.174267Z",
          "iopub.status.idle": "2024-07-08T05:36:12.181719Z",
          "shell.execute_reply.started": "2024-07-08T05:36:12.17424Z",
          "shell.execute_reply": "2024-07-08T05:36:12.180626Z"
        },
        "trusted": true,
        "id": "X6QEh-e7a0_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['processed_text'] = train_data['text'].apply(preprocess_text)\n",
        "test_data['processed_text'] = test_data['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:36:12.183039Z",
          "iopub.execute_input": "2024-07-08T05:36:12.183565Z"
        },
        "trusted": true,
        "id": "t5jx0Ru3a0_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "kYlm5BXqa0_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=train_data['processed_text'], vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AT1qn05_a0_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create document vectors\n",
        "def document_vector(doc):\n",
        "    doc_vector = np.zeros(100)  # 100 is the vector size we chose for Word2Vec\n",
        "    count = 0\n",
        "    for word in doc:\n",
        "        if word in w2v_model.wv:\n",
        "            doc_vector += w2v_model.wv[word]\n",
        "            count += 1\n",
        "    if count != 0:\n",
        "        doc_vector /= count\n",
        "    return doc_vector\n",
        "\n",
        "# Create document vectors for train and test data\n",
        "X_train = np.array(train_data['processed_text'].apply(document_vector).tolist())\n",
        "X_test = np.array(test_data['processed_text'].apply(document_vector).tolist())"
      ],
      "metadata": {
        "trusted": true,
        "id": "T8aX5sW1a0_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_data['label']\n",
        "y_test = test_data['label']"
      ],
      "metadata": {
        "trusted": true,
        "id": "3ziKo5vNa0_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "train_data['label'].value_counts().plot(kind='bar')\n",
        "plt.title('Sentiment Distribution in Training Data')\n",
        "plt.xlabel('Sentiment (0: Negative, 1: Positive)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "4SWRXfwaa0_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}